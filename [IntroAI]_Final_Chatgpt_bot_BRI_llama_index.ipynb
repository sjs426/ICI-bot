{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "mkNP0CxTutpt",
        "m0UIk519uKMw",
        "pw7MgA3NHWH_"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pingfaisnuli/ICI-s_chatbot/blob/main/%5BIntroAI%5D_Final_Chatgpt_bot_BRI_llama_index.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reference \n",
        "# https://llamahub.ai/l/file\n",
        "# https://gpt-index.readthedocs.io/en/stable/guides/primer/usage_pattern.html\n",
        "# https://github.com/jerryjliu/llama_index/issues/1900\n",
        "# https://github.com/jerryjliu/llama_index"
      ],
      "metadata": {
        "id": "88NxPD3eIM-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#https://zhuanlan.zhihu.com/p/613155165\n",
        "#https://www.wbolt.com/building-a-chatbot-based-on-documents-with-gpt.html\n",
        "\n",
        "!pip install llama-index\n",
        "from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, Document, LLMPredictor , PromptHelper, ServiceContext, GPTListIndex"
      ],
      "metadata": {
        "id": "a59EVtGvAyfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "j7GhO6ZwaOHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqCFfvGIAshz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = 'Fill in your API'\n",
        "documents = SimpleDirectoryReader('/content/drive/MyDrive/IntroAI/FAQs.txt').load_data()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.node_parser import SimpleNodeParser\n",
        "\n",
        "parser = SimpleNodeParser()\n",
        "\n",
        "nodes = parser.get_nodes_from_documents(documents)"
      ],
      "metadata": {
        "id": "VheI03jm_wvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import GPTVectorStoreIndex\n",
        "\n",
        "index = GPTVectorStoreIndex.from_documents(documents)"
      ],
      "metadata": {
        "id": "MOM5qBFw_zFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import StorageContext\n",
        "\n",
        "storage_context = StorageContext.from_defaults()\n",
        "storage_context.docstore.add_documents(nodes)\n",
        "\n",
        "index1 = GPTVectorStoreIndex(nodes, storage_context=storage_context)\n",
        "index2 = GPTListIndex(nodes, storage_context=storage_context)"
      ],
      "metadata": {
        "id": "XhQXBfUVAH0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import GPTVectorStoreIndex\n",
        "\n",
        "index = GPTVectorStoreIndex([])\n",
        "for doc in documents:\n",
        "    index.insert(doc)"
      ],
      "metadata": {
        "id": "oftt-Qh-APC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index import LLMPredictor, GPTVectorStoreIndex, PromptHelper, ServiceContext\n",
        "from langchain import OpenAI\n",
        "\n",
        "# define LLM\n",
        "llm_predictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n",
        "\n",
        "# define prompt helper\n",
        "# set maximum input size\n",
        "max_input_size = 4096\n",
        "# set number of output tokens\n",
        "num_output = 256\n",
        "# set maximum chunk overlap (e.g., 20% overlap)\n",
        "max_chunk_overlap = 0.2\n",
        "prompt_helper = PromptHelper(max_input_size, num_output, max_chunk_overlap)\n",
        "\n",
        "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
        "\n",
        "index = GPTVectorStoreIndex.from_documents(\n",
        "    documents, service_context=service_context\n",
        ")\n",
        "\n",
        "\n",
        "from llama_index import (\n",
        "    GPTVectorStoreIndex,\n",
        "    ResponseSynthesizer,\n",
        ")\n",
        "from llama_index.retrievers import VectorIndexRetriever\n",
        "from llama_index.query_engine import RetrieverQueryEngine\n",
        "from llama_index.indices.postprocessor import SimilarityPostprocessor\n",
        "\n",
        "# configure retriever\n",
        "retriever = VectorIndexRetriever(\n",
        "    index=index, \n",
        "    similarity_top_k=2,\n",
        ")\n",
        "\n",
        "# configure response synthesizer\n",
        "response_synthesizer = ResponseSynthesizer.from_args(\n",
        "    node_postprocessors=[\n",
        "        SimilarityPostprocessor(similarity_cutoff=0.7)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# assemble query engine\n",
        "query_engine = RetrieverQueryEngine(\n",
        "    retriever=retriever,\n",
        "    response_synthesizer=response_synthesizer,\n",
        ")\n",
        "\n",
        "# define a list of inappropriate or offensive words and phrases\n",
        "inappropriate_content = [\"bad word\", \"offensive phrase\"]\n",
        "\n",
        "# get user query\n",
        "user_query = input(\"Enter your question: \")\n",
        "\n",
        "# check if user query contains any inappropriate or offensive content\n",
        "if any(content in user_query for content in inappropriate_content):\n",
        "    # inform user that their question is not supported\n",
        "    print(\"Sorry, your question is not supported\")\n",
        "else:\n",
        "    # pass query to query engine\n",
        "    response = query_engine.query(user_query)\n",
        "    if not response:\n",
        "        print(\"Sorry, I can't answer this\")\n",
        "    else:\n",
        "        print(response)\n"
      ],
      "metadata": {
        "id": "4ZYk2XD4Kuo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "id": "TztijZxc5Da1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def answer_question(question):\n",
        "    # define a list of inappropriate or offensive words and phrases\n",
        "    inappropriate_content = [\"Who are you\", \"offensive phrase\"]\n",
        "    \n",
        "    # check if user query contains any inappropriate or offensive content\n",
        "    if any(content in question for content in inappropriate_content):\n",
        "        # inform user that their question is not supported\n",
        "        return \"Sorry, your question is not supported\"\n",
        "    else:\n",
        "        # build index\n",
        "        index = GPTVectorStoreIndex.from_documents(\n",
        "        documents, service_context=service_context)\n",
        "\n",
        "        # configure retriever\n",
        "        retriever = VectorIndexRetriever(\n",
        "            index=index, \n",
        "            similarity_top_k=2,\n",
        "        )\n",
        "\n",
        "        # configure response synthesizer\n",
        "        response_synthesizer = ResponseSynthesizer.from_args(\n",
        "            node_postprocessors=[\n",
        "                SimilarityPostprocessor(similarity_cutoff=0.7)\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # assemble query engine\n",
        "        query_engine = RetrieverQueryEngine(\n",
        "            retriever=retriever,\n",
        "            response_synthesizer=response_synthesizer,\n",
        "        )\n",
        "\n",
        "        # query\n",
        "        response = query_engine.query(question)\n",
        "        return response\n",
        "\n",
        "iface = gr.Interface(fn=answer_question, inputs=\"text\", outputs=\"text\", title=\"Question Answering System\")\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "jwrZ5yFzLtbU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
